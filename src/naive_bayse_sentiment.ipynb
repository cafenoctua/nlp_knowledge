{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴抽出を改良による極性判定の精度向上確認\n",
    "\n",
    "scikit-learnを用いて自然言語の極性判定を実装します。また、Bag of Wordsにおける特徴の表現方法によって正解率が変化することを確認します。\n",
    "今回はテストデータは用いず、10分割交差検証による正解率の平均を確認します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "\n",
    "今回用いるデータセット「MovieReview」の取得です。パッケージを用いてデータセットをダウンロードします。\n",
    "\n",
    "取得できるデータの種類についての説明と提供元のURLが確認できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Moview Review Data\n",
      "movie review data is annotated by 3 kinds of label (polarity, subjective rating, subjectivity).\n",
      "see also: http://www.cs.cornell.edu/people/pabo/movie-review-data/\n"
     ]
    }
   ],
   "source": [
    "import chazutsu\n",
    "chazutsu.datasets.MovieReview.polarity().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードされたデータはローカルに展開されると同時に、\n",
    "シャッフル・訓練データとテストデータの分割されます。\n",
    "また、pandas形式で読込を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin downloading the Moview Review Data dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz.\n",
      "The dataset is saved to /media/brunolw/D/knowledge/nlp_knowledge/src/data/moview_review_data_polarity/review_polarity.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b4ddb74e674caaaf6c525854c7fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3053.943359375), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting negative data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6064f04ce64614a1f60ee437ff1b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting positive data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0d7d6716bb46f4b9a4953dc123d889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shuffle the extracted dataset.\n",
      "Done all process! Make below files at /media/brunolw/D/knowledge/nlp_knowledge/src/data/moview_review_data_polarity\n",
      " review_polarity.txt\n"
     ]
    }
   ],
   "source": [
    "r = chazutsu.datasets.MovieReview.polarity().download(force=True, test_size=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードされたデータを先頭から5件確認します。\n",
    "レビュー内容が「review」、極性が「polarity」として「0(ネガティブ)」、「1(ポジティブ)」として格納されています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bruce willis is a type-casted actor . in die h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>written by david j . schow and john shirley , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>in tim burton's `sleepy hollow' , there is a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>after the recent animated debacles of , \" a ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>reading the cast and director for the new mobs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                             review\n",
       "0         1  bruce willis is a type-casted actor . in die h...\n",
       "1         0  written by david j . schow and john shirley , ...\n",
       "2         1  in tim burton's `sleepy hollow' , there is a m...\n",
       "3         0  after the recent animated debacles of , \" a ru...\n",
       "4         0  reading the cast and director for the new mobs..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.data().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(r.data()['review'], r.data()['polarity'], test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 1264    as with his other stateside releases , jackie ...\n",
       "722     deep rising is one of \" those \" movies . the k...\n",
       "210     plot : a human space astronaut accidentally fa...\n",
       "252     senseless ( r ) marlon wayans is a very talent...\n",
       "297     the heartbreak kid ( reviewed on aug . 26th/19...\n",
       "                              ...                        \n",
       "1122    denzel washington is among the many actors thi...\n",
       "1346    one of the more unusual and suggestively viole...\n",
       "1406    some critics , including siskel & ebert , are ...\n",
       "1389    ok , i admit it--i find camp amusement with th...\n",
       "1534    notting hill's trailer is awful : a laughless ...\n",
       "Name: review, Length: 1600, dtype: object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成\n",
    "\n",
    "特徴抽出とモデル選択をパイプラインとして作成します。\n",
    "今回はCountVectorizerとでBag of Words形式に変換します。（TfidfTransformerでtfidf重み付けも行っています）\n",
    "\n",
    "単語の数え方における「単語まとまりの単位(1単語 or 2単語)」、「数え方の表現(N回 or 出現有無)」を組み合わせて4パターンのモデルを用意し、精度を比較します。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1単語・N回モデル\n",
    "\n",
    "文書の特徴量として単語まとまりの単位は1単語(unigram)、単語の出現回数をN回としてカウントするBoWを用いるモデル。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_pipeline_unigram_multicount():\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(token_pattern=r'[A-Za-z_]+')),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    return text_clf\n",
    "\n",
    "text_clf_unigram_multicount = build_pipeline_unigram_multicount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1単語・出現有無モデル\n",
    "\n",
    "文書の特徴量として単語まとまりの単位は1単語(unigram)、単語の出現回数を出現有無としてカウントするBoWを用いるモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline_unigram_binarycount():    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(binary=True, token_pattern=r'[A-Za-z_]+'),),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    return text_clf\n",
    "\n",
    "text_clf_unigram_binarycount = build_pipeline_unigram_binarycount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2単語・N回モデル\n",
    "\n",
    "文書の特徴量として単語まとまりの単位は2単語(unigram)、単語の出現回数をN回としてカウントするBoWを用いるモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline_bigram_multicount():    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(2,2), token_pattern=r'[A-Za-z_]+'),),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    return text_clf\n",
    "\n",
    "text_clf_bigram_multicount = build_pipeline_bigram_multicount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2単語・出現有無モデル\n",
    "\n",
    "文書の特徴量として単語まとまりの単位は2単語(unigram)、単語の出現回数を出現有無としてカウントするBoWを用いるモデル。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline_bigram_binarycount():    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(2,2), binary=True,token_pattern=r'[A-Za-z_]+'),),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    return text_clf\n",
    "\n",
    "text_clf_bigram_binarycount = build_pipeline_bigram_binarycount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validationによる正解率確認\n",
    "\n",
    "作成した4つのモデルの正解率を10分割交差検証で確認します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1単語・N回モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80625, 0.85   , 0.75625, 0.7625 , 0.8125 , 0.80625, 0.8375 ,\n",
       "       0.8375 , 0.80625, 0.79375])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(text_clf_unigram_multicount, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807 (+/- 0.058)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1単語・出現有無モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81875, 0.86875, 0.775  , 0.775  , 0.83125, 0.81875, 0.825  ,\n",
       "       0.84375, 0.86875, 0.7875 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(text_clf_unigram_binarycount, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.821 (+/- 0.065)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2単語・N回モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73125, 0.8    , 0.70625, 0.71875, 0.76875, 0.7625 , 0.80625,\n",
       "       0.825  , 0.80625, 0.7375 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(text_clf_bigram_multicount, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.766 (+/- 0.079)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2単語・出現有無モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76875, 0.83125, 0.7375 , 0.73125, 0.8    , 0.7875 , 0.825  ,\n",
       "       0.8    , 0.8375 , 0.725  ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(text_clf_bigram_binarycount, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784 (+/- 0.080)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
