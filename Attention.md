# Attentionとは
ある層の入力に対してどこにAttention(注意)するか決定し効率的かつ高精度な学習できるようにする機構です。
Attentionを使うことによって層の結合を密結合から疎結合にすることができ過学習が発生しづらいNNをつくれるため結果的に少ないデータでも過学習なく学習を行うことが可能となります。

Attentionはベクトルであり、多くの場合、ソフトマックス関数を使用したdense層の出力です。
Attentionを取り入れる前は全文(数百に及ぶ単語)を固定長のベクトルとして扱うため情報の損失や翻訳の精度を向上できませんでした。
Attentionはそのような問題を部分ながら解決することができます。

Attentionは適切な箇所であればどこでも繋げることができ精度向上に貢献します。

# なぜAttentionか
確率的言語モデルはマルコフ過程をベースに文を確率的に割り当てています。
異なる数の単語で構成される文の性質により、RNNは、単語間の条件付き確率をモデル化するために導入されます。

シンプルなRNNはモデリングの問題:
- 入出力が固定されてしまい現実世界で取り扱うような可変長の入出力と合いません。
- 文が長い場合は、勾配消失/爆発のため学習できないことがしばしばあります。

この問題への対策としてGRUやLSTMが提案されシンプルなRNN層に取って代わりました。
これにより、埋め込み層で密ベクトルを計算効率のため生成されそのベクトルを使ってGRUをエンコーダーに通し計算された隠れ層のパラメータを順次引き継ぐことで左から右にかけての情報を持ってデータをエンコーディングし続けてデコーディングしていきます。

ただ、上記の方法で学習される場合の問題としてエンコードされた最終的パラメータは一つとなり隠れ層のパラメータの情報の欠落が防げていないという点です。

そのための対策としてAttentionがあります。

# Attetionの動作
基本的なエンコーダー-デコーダーアーキテクチャと同様で特徴的なメカニズムは、コンテキストベクトルをエンコーダーとデコーダーの間のギャップに接続します。
コンテキストベクトルは、すべてのセルの出力を入力として受け取り、生成する単一の単語デコーダーごとにソース言語の単語の確率分布を計算します。
このメカニズムを利用することにより、デコーダーは、1つの隠れた状態に基づいて推測するだけでなく、ある程度グローバルな情報をキャプチャすることができます。

固定ターゲットワードの場合、最初に、すべてのエンコーダーの状態をループして、ターゲット状態とソース状態を比較し、エンコーダーの各状態のスコアを生成します。
次に、softmaxを使用してすべてのスコアを正規化すると、ターゲットの状態を条件とする確率分布が生成されます。
最後に、コンテキストベクトルのトレーニングを容易にするために重みが導入されました。

1. デコード中、コンテキストベクトルは出力ワードごとに計算されます。したがって、サイズがターゲット単語の数にソース単語の数を掛けたものである2Dマトリックスが作成されます。式（1）は、1つのターゲットワードと一連のソースワードが与えられた場合に単一の値を計算する方法を示しています。
2. コンテキストベクトルが計算されると、Attentionベクトルは、コンテキストベクトル、ターゲットワード、およびAttetion関数fによって計算できます。
3. 訓練可能であるためには注意メカニズムが必要です。式（4）によると、どちらのスタイルもトレーニング可能な重みを提供します（LuongのW、BahdanauのW1およびW2）。したがって、スタイルが異なれば、パフォーマンスも異なる可能性があります。

# まとめ
現在多くのバリエーションが研究されておりスコア関数とattention関数、またはソフトアテンションとハードアテンション(微分可能かどうか)で異なります。
ですが基本的構造は上述したようなものです。

# Ref
- [A Brief Overview of Attention Mechanism](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)