# Attentionとは
ある層の入力に対してどこにAttention(注意)するか決定し効率的かつ高精度な学習できるようにする機構です。
Attentionを使うことによって層の結合を密結合から疎結合にすることができ過学習が発生しづらいNNをつくれるため結果的に少ないデータでも過学習なく学習を行うことが可能となります。
