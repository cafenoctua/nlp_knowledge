# なぜ単語や文書を数学的な表現に落とし込む必要があるのか
言語データを活用した何らかのシステムを作る場合にシステムを動かすコンピューターでは、デジタルの表現が必須のため言語データを数学的な数値に変換する必要が出て来ます。

つまり、現在用いられている文字変換や検索、chatbotのような発展系でも言語が数値変換されている必要がそれら技術の基礎もっと言えば前処理や統計的手法を行うために不可欠な手法なのです。

ここでは、その言語→数値変換の導入と具体的な手法と実際にどう行っていくかをpythonのコードも活用しつつ説明します。

# 言語の数値変換手法
- bag-of-words
- one-hot表現
- 言語モデル
  - 文や文書の生成確率モデルに文字を数値化する
- 分散表現(埋め込み表現)

## 単語分散表現
- Word2Vec
- FastText
- GloVe
- ELMo
- BERT

## 文書分散表現(文章ベクトル)
### 教師なし
- tf-idf
- SIF
- Universal Setence Encoder
- WRD

### 教師あり
- SentenceBERT
- SCDV
- RNN

# bag-of-words
- 単語の頻度を計算することで文書内での単語の頻度の位置づけから解析が可能となります。
- 変種もいくつか存在し、文書に特定単語が存在する場合1、しない場合は0という二値ベクトルのパターンも存在します
- n-gramを用いたbag-of-ngramsというものを存在します。
  - n-gramを用いればnに設定した数値分の語順を情報として持つことができる
- デメリット
  - 単語の順序のような情報を捨ててしまいます。
  - 大量の単語を要素としてベクトルを作るとデータスパースネス問題が発生しやすくなます。
    - データスパースネスとはベクトルの大部分が0で埋まってしまっている状態を指します。
  
数学的表現(ベクトル表現)した状態の各要素を素性もしくは特徴量と呼びます。

# 分散表現
単語もしくは文章を特定次元数のベクトルで表現する手法です。
ニューラルネットワーク(以下、NN)を使った手法では入力として分散表現が用いられます。
分散表現は、ベクトル化した単語、文章の意味よく捉えれば捉えるほどにNNタスクでの性能向上が見込まれます。
例えば、BERTのような大量のラベルの無い言語データを学習させた状態を初期値として少量のラベル付きデータを使ったFine Tuningを行われます。


## 課題
- 実際のタスクに使用すると期待通り性能が向上しない
  - 分散表現の評価方法に問題があるようで、その評価方法とは多くの場合、人が作成した単語類似度の評価セットとの相関度で評価されるためそのデータ・セットで評価が良くても実際に用いられるデータ・セットが想定した形とは限らないためです。
  - また、作成される大体の評価データ・セットでは単語の類似性と関連性を区別していないため、例えば、(conputer、keyboard)を関連しているけれど類似しないというようなことがあります。
    - 区別しているデータ・セットを使うと実際のタスクでの性能に正の相関が見れらます。
- 単語の曖昧性を考慮していない
  - 単語の多義性を考慮出来ていないため同じ単語であれば意味が違えど同じ一つのベクトルとして表されます。

# 数値変換を行う前の前処理
言語データの形式は様々で単純な文章からHTMLのようなweb上から取得したようなデータまで存在します。

文書として共通で発生する事象としては同単語の高頻度の出現や意味のないタグ等文書の話題とは直接的に関連のないもの**ストップワード**というものが存在します。
このストップワードはデータの分布や学習に影響を与えるため基本的は削除する必要があります。

また、単語の品詞変換により同様な意味(派生語)を指すものを同一の素性に修正する方法をステミングといいます。
だが、実際にステミングを行うに文脈を読み込む必要があり労力がかかるためよく用いられるステミング手法として**ポーターのステマー**(英語文書の場合)というものが用いられます。

さらに細かく単語を分類する方法としては**品詞タグ付け**と呼ばれる手法もありこれにより同様な単語でも品詞違いで素性を別として学習させることが可能です。
これを行うことで**語義の曖昧性解消**の効果が期待できます。


また、日本語の場合は単語が英語のように明確に区切られていないためその処理が必要となります。
手法としては、単語区切りと品詞タグ付けを同時に行う**形態素解析**が有名です。

- 形態素解析のツール
  - MeCab
  - ChaSen
さらに、日本語では上述しましたステミング処理を細かく行えないため単語を単一の素性とみなした見出し語化が行われることがあります。

# 単語分散表現の最適な次元数について

# Ref
- 言語処理のための機械学習入門
- ゼロから作るDeepLearning②自然言語処理編
- [awesome-sentence-embedding](https://github.com/Separius/awesome-sentence-embedding)
- [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)
- [【まとめ】自然言語処理における単語分散表現（単語ベクトル）と文書分散表現（文書ベクトル](https://qiita.com/kenta1984/items/6dc327c31b6c36e66863)
- [なぜ自然言語処理にとって単語の分散表現は重要なのか？](https://qiita.com/Hironsan/items/a58636f946dd51f670b0)
- [単語分散表現の最適な次元数を決めるための指針](https://qiita.com/Hironsan/items/01fd880f1522e2025a78)
- [絵で理解するWord2vecの仕組み](https://qiita.com/Hironsan/items/11b388575a058dc8a46a)