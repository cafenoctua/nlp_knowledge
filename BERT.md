# なぜBERTが必要か?
LSTMやBiLSTMでは単語分散表現をうまく獲得できますがあくまで単語単体のため文脈は考慮されていませんでした。
そのため多義語のような一つの単語に複数意味が含まれ、文脈によって意味が決定される単語を必ずしも正しく認識できるわけではありません。
そのような問題への対策として、文中の他の単語の情報も使って単語の分散表現を獲得する手法BERTが作られました。
これによりBiLSTMより発展した前後の文脈情報を扱い単語の分散表現を獲得できるようなりました。

では、文脈を使った単語の分散表現とはどういったものになるでしょうか?
例えば、
- I like Apple products.
- I like apple juice.
この２つの文のappleというのは会社名と果実と全く異なるものを表しています。
ですが、私達はこの違いを文脈からを読み取ることで判別することができるためこのように判別できるように文脈を含む単語の分散表現でより良い表現を得ることができます。

他にも文脈を扱ったモデルはありますが現在はBERTが主流となっています。
BERTは双方向かつ深いネットワークで構築することで良い表現を獲得しています。
BERTでは2ステップの学習を経て学習済みのモデルとなります。
- 事前学習
- Fine-tuning

事前学習でラベル無しデータを大量に学習させ**マスクされた単語学習**を行うことで文脈からの単語予測と与えた**2つの文が隣接文か否か**を予測するタスクを行うことで文脈を使った単語の分散表現と文の関係的に近いかを学習させることができます。

Fine-tuningは少量のラベルデータを使って事前学習済みのBERTに対して解かせたいタスクへの学習を行えます。
こうすることで目的に応じた少量のタスクを事前に汎化性能を獲得したモデルをあてがう事ができ高い精度が期待できます。


